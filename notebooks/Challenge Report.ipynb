{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alejandro.jimenez/Documents/reference_repos/titanic-ml-model\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m titanic_ml_model train --train_data ./data/raw/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Model Performance Results!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "<h5>\n",
       "The model achieved the following on the test data (20%)\n",
       "\n",
       " - Accuracy of **84.92%**\n",
       "\n",
       " - F1 score of **81.38%**\n",
       "\n",
       " - Precision of **83.10%**\n",
       "\n",
       " - Recall of **79.73%**\n",
       "\n",
       "Based on the current Challenge LeaderBoard, that will locate us within the Top 185/15860, per positioning by May 31st 2023\n",
       "\n",
       "The F1 score, that reflects the harmony between precision and recall, of **81.38%** is fairly high.\n",
       "\n",
       "Both values from Precision and Recall show a **well balanced and robust model**. As a result of the Cross Validation and FineTuning Steps taken.\n",
       "\n",
       "</h5>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "with open(\"./logs/logs.txt\", \"r\") as file:\n",
    "    last_exec = file.readlines()[-1]\n",
    "    logged_metrics = eval(last_exec.replace(\"\\n\", \"\"))\n",
    "\n",
    "variable = \"world\"\n",
    "display(Markdown(f\"# Model Performance Results!\"))\n",
    "display(Markdown(f\"\"\"\n",
    "<h5>\n",
    "The model achieved the following on the test data (20%)\n",
    "\n",
    " - Accuracy of **{logged_metrics['accuracy']:.2%}**\n",
    "\n",
    " - F1 score of **{logged_metrics['f1_score']:.2%}**\n",
    "\n",
    " - Precision of **{logged_metrics['precision']:.2%}**\n",
    "\n",
    " - Recall of **{logged_metrics['recall']:.2%}**\n",
    "\n",
    "Based on the current Challenge LeaderBoard, that will locate us within the Top 185/15860, per positioning by May 31st 2023\n",
    "\n",
    "The F1 score, that reflects the harmony between precision and recall, of **{logged_metrics['f1_score']:.2%}** is fairly high.\n",
    "\n",
    "Both values from Precision and Recall show a **well balanced and robust model**. As a result of the Cross Validation and FineTuning Steps taken.\n",
    "\n",
    "</h5>\n",
    "\"\"\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "\n",
    "The SHAP values provided give us a sense of the importance of each feature in the model. \n",
    "\n",
    "The higher the **SHAP value**, the more impact that feature has on the model's output. \n",
    "\n",
    "From the provided SHAP values, the most influential features appear to be 'fare', 'pclass', and 'name_title', and age. \n",
    "\n",
    "This suggests that:\n",
    " - **Fare paid by a passenger**\n",
    " - **Passenger class**\n",
    " - **Title** (Mr, Miss, Master, etc, which can indicate social status and embeds sex attributes) \n",
    " - **Age**\n",
    "\n",
    "All are particularly important in predicting survival on the Titanic.\n",
    "\n",
    "SHAP Summary Plot is shown below:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Shap Summary Plot Feature Importance](../logs/shap_summary_plot.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment Steps\n",
    "\n",
    "There are several ways to put the current model into production.\n",
    "\n",
    "Here we will outline the steps for a **Microservice type Architecture** \n",
    " \n",
    "so that the model can be consumed through an API by any client (webpage, app, etc). \n",
    " \n",
    " We decide for this architecture for the kind of model this package involves (there is no need for a batch prediction to be saved weekly as it is a past event, \n",
    " but people might want to play their chances of surviving by playing with data, thus an app, webpage, etc).\n",
    "\n",
    "\n",
    "1) **Dockerization**: Create a Dockerfile that describes the environment in which your ML model runs. This makes sure we can later upload the Docker Image in different versions of a Registry such as Amazon Elastic Container Registry (ECR).\n",
    "\n",
    "2) **Uploading it to a Registry**: Such as Amazon Elastic Container Registry (ECR)\n",
    "\n",
    "3) **Model Deployment (HTTP endpoint)**: This can be achieved by:\n",
    "    - Modifying this package to work as a Flask RESTFul API, and then use Amazon Elastic Container Service for orchestration. **(Microservice Architecture)**\n",
    "    - Using a serverless alternative such as with Amazon SageMaker that allows serverless support for Docker Images in ECR. It provides an endpoint for requests to be made.\n",
    "\n",
    "4) **Scaling**: Amazon API Gateway can be used to create a scalable API for a model hosted in Amazon SageMaker. Otherwise a load balancer option is good to consider if you are expected to handle a large amount of requests.\n",
    "\n",
    "5) **CI/CD**: For continuous updates and deployments it is good to consider a pipelining options with technologies such as Jenkins. Some services in AWS help automate all the steps above with AWS CodePipeline (for continuous delivery).\n",
    "\n",
    "6) **Monitoring**: It is key to set up steps to monitor the ML model's performance and usage. AWS provides services such as Amazon CloudWatch and X-Ray (I have yet to explore). Alternatively, you could integrate saving metrics steps in the package itself and use a BI tool to visualize performance (which is the current plan for us in SP&A to have a metric monitoring and A/B monitoring for model use cases).\n",
    "\n",
    "6) **Security**: This might be optional provided the application or endpoint requires a certain type of authentication. Services in AWS may help with this but I do not have full clarity on this.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
